{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joints_vecs.shape: (116, 263)\n",
      "joints.shape: (116, 22, 3)\n",
      "mean.shape: (263,)\n",
      "std.shape: (263,)\n",
      "[[ 0.          0.94151205  0.        ]\n",
      " [ 0.06232133  0.86118996 -0.01699019]\n",
      " [-0.05533443  0.84660774 -0.00237302]\n",
      " [-0.00556196  1.0656054  -0.04336009]\n",
      " [ 0.15537223  0.47988468 -0.04677429]\n",
      " [-0.11954129  0.46223748 -0.02196455]\n",
      " [ 0.00668192  1.2051041  -0.01346632]\n",
      " [ 0.14500111  0.04888482 -0.08038796]\n",
      " [-0.09412326  0.04250382 -0.08791802]\n",
      " [ 0.004126    1.2616646  -0.00424177]]\n",
      "tensor([[ 0.0000,  0.9415,  0.0000],\n",
      "        [ 0.0623,  0.8612, -0.0170],\n",
      "        [-0.0553,  0.8466, -0.0024],\n",
      "        [-0.0056,  1.0656, -0.0434],\n",
      "        [ 0.1554,  0.4799, -0.0468],\n",
      "        [-0.1195,  0.4622, -0.0220],\n",
      "        [ 0.0067,  1.2051, -0.0135],\n",
      "        [ 0.1450,  0.0489, -0.0804],\n",
      "        [-0.0941,  0.0425, -0.0879],\n",
      "        [ 0.0041,  1.2617, -0.0042]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from data_loaders.humanml.scripts.motion_process import recover_from_ric\n",
    "from model.rotation2xyz import Rotation2xyz\n",
    "import torch\n",
    "def ric2pos(sample):\n",
    "    rot2xyz = Rotation2xyz(device='cpu', dataset='humanml')\n",
    "    n_joints = 22 if sample.shape[1] == 263 else 21\n",
    "    rot2xyz_pose_rep = 'xyz' \n",
    "    rot2xyz_mask = None \n",
    "    \n",
    "    sample = recover_from_ric(sample, n_joints)\n",
    "    sample = rot2xyz(x=sample, mask=rot2xyz_mask, pose_rep=rot2xyz_pose_rep, glob=True, translation=True,\n",
    "                            jointstype='smpl', vertstrans=True, betas=None, beta=0, glob_rot=None,\n",
    "                            get_rotations_back=False)\n",
    "    return sample\n",
    "\n",
    "id = 0\n",
    "\n",
    "\n",
    "joints = np.load(f'/home/yhc/momask-codes/dataset/HumanML3D/new_joints/{id:06d}.npy')\n",
    "joints_vecs = np.load(f'/home/yhc/momask-codes/dataset/HumanML3D/new_joint_vecs/{id:06d}.npy')\n",
    "mean = np.load(f'/home/yhc/momask-codes/dataset/HumanML3D/Mean.npy')\n",
    "std = np.load(f'/home/yhc/momask-codes/dataset/HumanML3D/Std.npy')\n",
    "print(f\"joints_vecs.shape: {joints_vecs.shape}\")\n",
    "print(f\"joints.shape: {joints.shape}\")\n",
    "print(f\"mean.shape: {mean.shape}\")\n",
    "print(f\"std.shape: {std.shape}\")\n",
    "# joints_vecs_inv = joints_vecs * std[None,...] + mean[None,...]\n",
    "\n",
    "joints_vecs_inv_recovered = ric2pos(torch.from_numpy(joints_vecs))\n",
    "time_step = 0\n",
    "joint_idx = [i for i in range(10)]\n",
    "print(joints[time_step][joint_idx])\n",
    "print(joints_vecs_inv_recovered[time_step][joint_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/yhc/OmniControl/dataset/t2m_mean.npy\")\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
